{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aAoX2U0khsYG"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, accuracy_score\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, cross_validate\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","#CLASSIFICATION RANDOM FOREST (ONE TO PREDICT HIGH/LOW VOLATILITY FOLLOWING DAY)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__SZxTfKhsYH"},"outputs":[],"source":["df_final = pd.read_csv('FINAL_1YEAR_DATA.csv',parse_dates=[\"date\"])\n","print(df_final.head())\n","print(df_final.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbQOQZBrhsYI"},"outputs":[],"source":["from sklearn.metrics import precision_score\n","#CLASSIFICATION RANDOM FOREST\n","#median threshold (kinda unrealistic, want to test other threshold decisions: )\n","#threshold = df_final[\"Target\"].median()\n","#df_final[\"Target_binary\"] = (df_final[\"Target\"] > threshold).astype(int)\n","\n","#QUANTILE BASED THRESHOLD\n","threshold = df_final[\"Target\"].quantile(0.7)\n","df_final[\"Target_binary\"] = (df_final[\"Target\"] >= threshold).astype(int)\n","#print(df_final[\"Target_binary\"].value_counts(normalize=True))\n","\n","\n","# Define features\n","features = [\n","    \"RealizedVol_3d\",\n","    \"reddit_sentiment_lag1\", \"reddit_volume_lag1\",\n","    \"news_sentiment_lag1\", \"news_volume_lag1\",\n","   \"reddit_sentiment_missing\", \"news_missing\"\n","]\n","\n","# Train-test split\n","X = df_final[features]\n","y = df_final[\"Target_binary\"]\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, shuffle=True, stratify=y, random_state=42\n",")\n","\n","# Train Random Forest USING GRID SEARCH\n","# param_grid = {\n","#     'n_estimators': [100,200,300,350],\n","#     'max_depth': [4, 6, 8, 10, None],\n","#     'min_samples_split': [2, 5, 10],\n","#     'min_samples_leaf': [1, 3, 5],\n","#     'class_weight': [None, 'balanced'],\n","#     'max_features': ['sqrt','log2',.5]\n","# }\n","\n","# rf = RandomForestClassifier(random_state=42, oob_score=True)\n","# grid_rf = HalvingGridSearchCV(\n","#     rf, param_grid,factor = 3,\n","#     cv=5, scoring='roc_auc', n_jobs=-1, verbose=2\n","# )\n","# grid_rf.fit(X_train, y_train)\n","# print(\"Best Random Forest params:\", grid_rf.best_params_)\n","# best_rf = grid_rf.best_estimator_\n","# sent_model = best_rf\n","\n","sent_model = RandomForestClassifier(n_estimators=300, max_depth=7, min_samples_leaf=1,max_features= 'log2',\n"," random_state=42)\n","sent_model.fit(X_train,y_train)\n","\n","#Get f1 for binary prediction at threshold for comparison later.\n","rf_probs = sent_model.predict_proba(X_test)[:, 1]\n","prob_thresh = np.quantile(rf_probs,0.7)\n","y_pred_rf_thresh = (rf_probs >= prob_thresh).astype(int)\n","\n","sentiment_rf_f1 = f1_score(y_test, y_pred_rf_thresh)\n","sentiment_rf_auc = roc_auc_score(y_test,rf_probs)\n","\n","# Evaluate\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf_thresh))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf_thresh))\n","print(\"F1 of Positive Class:\", sentiment_rf_f1)\n","\n","\n","#TEST TRADING STRATEGY:\n","#SIGNALS MEAN DATA SAMPLE TRIGGERED HIGH CONFIDENCE OR HIGH VOLATILITY SIGNAL 151 TIMES.\n","#PRECISION MEANS OF THOSE SET OF SIGNALS S, X% ACTUALLY CLASSIFIED CORRECTLY.\n","\n","\n","\n","# Decision: Enter trade if prob of high volatility > threshold\n","X_test_sim = X_test.copy()\n","X_test_sim[\"vol_prob\"] = rf_probs\n","X_test_sim[\"true_label\"] = y_test.values\n","X_test_sim[\"signal\"] = (X_test_sim[\"vol_prob\"] >= prob_thresh).astype(int)\n","\n","# How many signals?\n","n_signals = X_test_sim[\"signal\"].sum()\n","print(f\"Trade Signals Triggered: {n_signals} out of {len(X_test_sim)} samples\")\n","\n","# How accurate?\n","precision = precision_score(X_test_sim[\"true_label\"], X_test_sim[\"signal\"])\n","print(f\"Precision of Strategy: {precision:.3f}\")\n","\n","# High confidence trade indexes\n","print(\"\\nTop 5 high-confidence trades:\")\n","print(X_test_sim.sort_values(\"vol_prob\", ascending=False).head(5)[[\"vol_prob\", \"true_label\"]])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yvtjUbRihsYI"},"outputs":[],"source":["#CLASSIFICATION ANALYSIS\n","train_scores = []\n","test_scores = []\n","f1_tests = []\n","f1_trains = []\n","depths = list(range(1,7, 1))\n","\n","for depth in depths:\n","    model = RandomForestClassifier(max_depth=depth, n_estimators=300, max_features='log2',min_samples_leaf=5, random_state=42)\n","    model.fit(X_train, y_train)\n","\n","    train_prob = model.predict_proba(X_train)[:,1]\n","    train_thr = np.quantile(train_prob,.7)\n","    test_prob  = model.predict_proba(X_test)[:,1]\n","    test_thr = np.quantile(test_prob,.7)\n","\n","    train_pred = (train_prob >= train_thr).astype(int)\n","    test_pred  = (test_prob  >= test_thr).astype(int)\n","    train_scores.append(accuracy_score(y_train, train_pred))\n","    test_scores.append(accuracy_score(y_test, test_pred))\n","\n","    f1 = f1_score(y_test,test_pred)\n","    f1_tr = f1_score(y_train,train_pred)\n","    f1_tests.append(f1)\n","    f1_trains.append(f1_tr)\n","\n","# Plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(depths, train_scores, label=\"Train Accuracy\", marker='o')\n","plt.plot(depths, test_scores, label=\"Test Accuracy\", marker='o')\n","plt.title(\"Train vs. Test Accuracy by Tree Depth\")\n","plt.xlabel(\"Max Tree Depth\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(depths, f1_trains, marker=\"o\", label=\"Train F1\")\n","plt.plot(depths, f1_tests, marker=\"x\", label=\"Test F1\")\n","plt.xlabel(\"Max Tree Depth\")\n","plt.ylabel(\"Score\")\n","plt.title(\"AVG Train/Test F1 Score vs Max Tree Depth\")\n","plt.grid(True)\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WToobP9ihsYJ"},"outputs":[],"source":["#CLASSIFICATION ANALYSIS\n","feature_importance = pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x=feature_importance.values, y=feature_importance.index)\n","plt.title(\"Feature Importances\")\n","plt.xlabel(\"Importance\")\n","plt.ylabel(\"Feature\")\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","\n","#Because we are dealing with class imbalance (we have only top 30% volatility counting as our class y = 1), F1/AUC harder to optimize. Means RF model is learning meaningful patterns from this data\n","#even though the sentiment features have very little weights. They are helping by solving edge cases in the tree (slight boost in ROC)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjwV_7AthsYJ"},"outputs":[],"source":["#COMPARE CLASSIFICATION TO BASELINE GARCH MODEL\n","df_garch = pd.read_csv(\"garch_volatility_predictions.csv\", parse_dates=[\"date\"])\n","start, end = \"2024-02-02\", \"2025-04-08\"\n","mask_garch = (df_garch['date'] >= start) & (df_garch['date'] <= end)\n","df_garch = df_garch.loc[mask_garch].reset_index(drop=True)\n","df_garch[\"ticker\"] = df_garch[\"ticker\"].str.upper().str.replace(\".\", \"-\", regex=False)\n","df_garch['garch_vol'] = df_garch['Rolling_GARCH_volatility %'].astype(float) / 100\n","\n","\n","X_test_full = X_test.copy()\n","X_test_full[\"date\"] = df_final.loc[X_test.index, \"date\"].values\n","X_test_full[\"ticker\"] = df_final.loc[X_test.index, \"ticker\"].values\n","\n","# Merge with GARCH\n","X_test_full = X_test_full.merge(df_garch[['date','ticker','garch_vol']], on=[\"date\", \"ticker\"], how=\"left\")\n","print(X_test_full[[\"date\", \"ticker\", \"garch_vol\"]].head())\n","\n","garch_vol = X_test_full['garch_vol']\n","garch_thresh = np.quantile(garch_vol,.7)\n","X_test_full[\"garch_pred\"] = (garch_vol >= garch_thresh).astype(int)\n","\n","# Random Forest\n","\n","# GARCH\n","garch_preds = X_test_full[\"garch_pred\"]\n","#garch_auc = roc_auc_score(y_test, garch_preds)\n","garch_f1  = f1_score(y_test, X_test_full[\"garch_pred\"])\n","garch_auc = roc_auc_score(y_test,garch_vol)\n","print(\" Random Forest F1: \", sentiment_rf_f1)\n","print(\"Random Forest AUC: \", sentiment_rf_auc)\n","print(\"\\n GARCH F1: \", garch_f1)\n","print('Garch AUC: ', garch_auc)\n","\n","\n","print(\"\\n Random Forest Report:\")\n","print(classification_report(y_test,y_pred_rf_thresh))\n","\n","print(\"\\n GARCH Report:\")\n","print(classification_report(y_test, X_test_full['garch_pred']))\n","\n","#PRECISION GOOD WHEN FALSE ALARMS ARE COSTLY (WRONG BET ON VOLATILITY)\n","#RECALL GOOD WHEN TRUE EVENTS ARE COSTLY (MISSING HIGH-VOLATILITY)\n","#f1 score: mean of precision/recall\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzUsJnWmhsYK"},"outputs":[],"source":["# Train baseline RF\n","rf_base = RandomForestClassifier(\n","    n_estimators=300,\n","    max_depth=7,\n","    min_samples_leaf=1,\n","    max_features=.5,\n","    random_state=42\n",")\n","rf_base.fit(X_train[['RealizedVol_3d']], y_train)\n","\n","# Predict on baseline features\n","base_rf_probs = rf_base.predict_proba(X_test[['RealizedVol_3d']])[:, 1]\n","base_rf_thresh = np.quantile(base_rf_probs,.7)\n","base_rf_pred = (base_rf_probs >= base_rf_thresh).astype(int)\n","base_rf_f1 = f1_score(y_test, base_rf_pred)\n","base_rf_auc = roc_auc_score(y_test,base_rf_probs)\n","\n","plt.bar([\"GARCH\", \"RF (base)\", \"RF Base + Sentiment\"], [garch_f1, base_rf_f1, sentiment_rf_f1])\n","plt.ylabel(\"Binary Avg F1 Score\")\n","plt.title(\"Model Comparison: GARCH vs Random Forests\")\n","plt.ylim(0.5, 1)\n","plt.show()\n","\n","plt.bar([\"GARCH\", \"RF (base)\", \"RF Base + Sentiment\"], [garch_auc, base_rf_auc, sentiment_rf_auc])\n","plt.ylabel(\"Avg ROC-AUC Score\")\n","plt.title(\"Model Comparison: GARCH vs Random Forests\")\n","plt.ylim(0.5, 1)\n","plt.show()\n","\n","print(\"Base RF f1: \", base_rf_f1)\n","print(\"Base RF AUC: \", base_rf_auc)\n","print(\"sent auc: \", sentiment_rf_auc)\n","print(\"sent f1: \", sentiment_rf_f1)\n","#Learned that both RF models beat GARCH. Sentiment didn't improve RF model. Reddit/news sentiment noisy/sparse. Some tickers don't have enough signals (show ticker distribution chart)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l74oaPL-hsYK"},"outputs":[],"source":["#WALK FORWARD TESTING RF SENTIMENT\n","def walk_forward_test(df, features, target_col, train_window=1000, test_window=100):\n","    results = []\n","    preds_all = []\n","    actuals_all = []\n","\n","    # Walk across the data\n","    for start in range(0, len(df) - train_window - test_window, test_window):\n","        train = df.iloc[start:start + train_window]\n","        test = df.iloc[start + train_window:start + train_window + test_window]\n","\n","        X_train, y_train = train[features], train[target_col]\n","        X_test, y_test = test[features], test[target_col]\n","\n","        model = RandomForestClassifier(\n","            n_estimators=300,\n","            max_depth=7,\n","            min_samples_leaf=1,\n","            max_features=.5,\n","            random_state=42\n","        )\n","        model.fit(X_train, y_train)\n","        probs = model.predict_proba(X_test)[:, 1]\n","        thresh = np.quantile(probs,.7)\n","        preds = (probs >= thresh).astype(int)\n","\n","        # Record performance\n","        acc = accuracy_score(y_test, preds)\n","        auc = roc_auc_score(y_test, probs)\n","\n","        results.append({\"start\": start, \"accuracy\": acc, \"auc\": auc})\n","\n","        preds_all.extend(preds)\n","        actuals_all.extend(y_test)\n","\n","    return results, np.array(preds_all), np.array(actuals_all)\n","\n","df_walk = df_final.sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n","\n","results, preds_all, actuals_all = walk_forward_test(\n","    df_walk,\n","    features=features,\n","    target_col=\"Target_binary\",\n","    train_window=2000,\n","    test_window=150\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mw9T6SSwhsYK"},"outputs":[],"source":["results_df = pd.DataFrame(results)\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(results_df[\"start\"], results_df[\"accuracy\"], label=\"Accuracy\", marker='o')\n","plt.plot(results_df[\"start\"], results_df[\"auc\"], label=\"ROC AUC\", marker='x')\n","plt.xlabel(\"Start Index of Window\")\n","plt.ylabel(\"Score\")\n","plt.title(\"Walk-Forward Testing Results\")\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","\n","#Class imbalance occurring/ slight model overconfidence at various indexes (accuracy > ROC AUC)\n","\n","#ACCURACY VARIES WILDLY OVER TIME (SIMILAR TO MARKET EFFECTS) (SOME TIME PERIODS WE HAVE GREAT SIGNAL ALIGNMENT (SEE IF ITS DURING EARNINGS TIME) )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_01MPlYWhsYK"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","\n","sent_features = [\n","    \"reddit_sentiment_lag1\", \"reddit_volume_lag1\",\n","    \"news_sentiment_lag1\",    \"news_volume_lag1\",\n","    \"reddit_sentiment_missing\",\"news_missing\"\n","]\n","\n","X_base = df_final[[\"RealizedVol_3d\"]]   # baseline: 3-day realized vol only\n","X_All= df_final[features]       # sentiment-only (raw)\n","X_sent = df_final[sent_features]\n","# PCA pipeline will also use X_sent\n","\n","# Define binary target with fixed 0.7 cutoff\n","y = (df_final[\"Target\"] >= threshold).astype(int)\n","\n","# Set up 5‑fold stratified CV\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","scoring = {\"AUC\": \"roc_auc\", \"F1\": \"f1\"}\n","\n","#Build pipelines\n","base_pipe = Pipeline([\n","    (\"rf\", RandomForestClassifier(\n","        n_estimators=300,\n","        max_depth=7,\n","        min_samples_leaf=1,\n","        max_features = .5,\n","        random_state=42\n","    ))\n","])\n","\n","sent_pipe = Pipeline([\n","    (\"scaler\", StandardScaler()),\n","    (\"rf\", RandomForestClassifier(\n","        n_estimators=300,\n","        max_depth=7,\n","        min_samples_leaf=1,\n","        max_features = .5,\n","        random_state=42\n","    ))\n","])\n","\n","preproc = ColumnTransformer([\n","    # scale & reduce sentiment → 3 components\n","    (\"sent_pca\", Pipeline([\n","         (\"scale\", StandardScaler()),\n","         (\"pca\",   PCA(n_components=3))\n","    ]), sent_features),\n","\n","    # pass through the single vol feature\n","    (\"vol_passthrough\", \"passthrough\", ['RealizedVol_3d'])\n","])\n","\n","# Full PCA+vol → RF pipeline\n","pca_vol_pipe = Pipeline([\n","    (\"feature_setup\", preproc),\n","    (\"rf\", RandomForestClassifier(\n","        n_estimators=300,\n","        max_depth=7,\n","        min_samples_leaf=1,\n","        max_features=0.5,\n","        random_state=42\n","    ))\n","])\n","# Cross-validate each pipeline for both AUC and F1\n","base_res = cross_validate(base_pipe, X_base, y, cv=cv,\n","                          scoring=scoring, return_train_score=False)\n","sent_res = cross_validate(sent_pipe, X_All, y, cv=cv,\n","                          scoring=scoring, return_train_score=False)\n","pca_vol_res = cross_validate(pca_vol_pipe,X_All,y,cv=cv,scoring=scoring, return_train_score=False\n",")\n","\n","\n","def print_results(name, res):\n","    aucs = res[\"test_AUC\"]\n","    f1s  = res[\"test_F1\"]\n","    print(f\"=== {name} ===\")\n","    print(\"ROC‑AUC per fold:\", np.round(aucs, 3))\n","    print(\"Mean ROC‑AUC:\", f\"{aucs.mean():.3f}\")\n","    print(\"F₁ per fold:   \", np.round(f1s, 3))\n","    print(\"Mean F₁:   \", f\"{f1s.mean():.3f}\\n\")\n","\n","\n","print_results(\"3DVol Only RF (Base)\", base_res)\n","print_results(\"Rf sentiment + Base\", sent_res)\n","print_results(\"PCA Sentiment + Base\", pca_vol_res)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j36C7QFGhsYL"},"outputs":[],"source":["#ONE HOT ENCODING ON TICKERS\n","# One-hot encode\n","ticker_dummies = pd.get_dummies(df_final[\"ticker\"], prefix=\"ticker\")\n","# Concatenate with original features\n","df_with_ticker = pd.concat([df_final, ticker_dummies], axis=1)\n","\n","# Add  sentiment + baseline vol + ticker dummies\n","final_features_with_ticker = features + list(ticker_dummies.columns)\n","\n","X = df_with_ticker[final_features_with_ticker]\n","y = (df_with_ticker[\"Target\"] >= df_with_ticker[\"Target\"].quantile(0.7)).astype(int)\n","\n","model = RandomForestClassifier(\n","    n_estimators=350,\n","    max_depth=8,\n","    min_samples_leaf=5,\n","    max_features='sqrt',\n","    class_weight='balanced',\n","    random_state=42\n",")\n","\n","encode_scores = cross_val_score(model, X, y, cv=cv, scoring=\"roc_auc\")\n","encode_scores2 = cross_val_score(model, X, y, cv=cv, scoring=\"f1\")\n","\n","print(\"Cross-validated AUC scores with ticker encoding:\", encode_scores)\n","print(\"Mean AUC:\", encode_scores.mean())\n","print(\"Cross-validated f1 scores with ticker encoding:\", encode_scores2)\n","print(\"Mean F1: \", encode_scores2.mean())\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPrPJnQihsYL"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n","\n","# TEST FINAL MODEL (sentiment, one hot encoding on tickers, realized_vol3d are features. TARGET IS CLASS y = 1 if predicted vol is 70% oR HIGHER, 0 otherwise)\n","depths = list(range(1, 8))\n","train_acc = []\n","test_acc = []\n","f1_scores = []\n","auc_scores = []\n","\n","for d in depths:\n","    model = RandomForestClassifier(\n","        n_estimators=300,\n","        max_depth=d,\n","        min_samples_leaf=5,\n","        max_features='sqrt',\n","        random_state=42\n","    )\n","    model.fit(X_train, y_train)\n","\n","    train_prob = model.predict_proba(X_train)[:,1]\n","    train_thr = np.quantile(train_prob,.7)\n","    test_prob  = model.predict_proba(X_test)[:,1]\n","    test_thr = np.quantile(test_prob,.7)\n","\n","    train_pred = (train_prob >= train_thr).astype(int)\n","    test_pred  = (test_prob  >= test_thr).astype(int)\n","\n","    f1 = f1_score(y_test,test_pred, zero_division=0)\n","    auc = roc_auc_score(y_test,test_prob)\n","    f1_scores.append(f1)\n","    auc_scores.append(auc)\n","\n","\n","    train_acc.append(accuracy_score(y_train, train_pred))\n","    test_acc.append(accuracy_score(y_test,test_pred))\n","\n","# Plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(depths, train_acc, marker='o', label=\"Train Accuracy\")\n","plt.plot(depths, test_acc, marker='x', label=\"Test Accuracy\")\n","plt.title(\"Train vs Test Accuracy (Random Forest + PCA)\")\n","plt.xlabel(\"Tree Depth\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(depths, f1_scores, marker=\"o\", label=\"F1 Score\")\n","plt.plot(depths, auc_scores, marker=\"x\", label=\"ROC AUC\")\n","plt.xlabel(\"Max Tree Depth\")\n","plt.ylabel(\"Score\")\n","plt.title(\"F1 Score and ROC AUC vs Max Tree Depth\")\n","plt.grid(True)\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","\n","# Fit final model\n","rf_final = RandomForestClassifier(\n","    n_estimators=300,\n","    max_depth=7,\n","    min_samples_leaf=5,\n","    max_features='sqrt',\n","    random_state=42\n",")\n","rf_final.fit(X_train, y_train)\n","\n","# Get importances\n","importances = rf_final.feature_importances_\n","feature_names = X_train.columns\n","\n","# Plot\n","feat_df = pd.DataFrame({\n","    \"feature\": feature_names,\n","    \"importance\": importances\n","}).sort_values(\"importance\", ascending=False).head(20)  # Top 20 to keep it readable\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x=\"importance\", y=\"feature\", data=feat_df, palette=\"mako\")\n","plt.title(\"Top 20 Feature Importances (Random Forest + PCA + Ticker)\")\n","plt.xlabel(\"Importance\")\n","plt.ylabel(\"Feature\")\n","plt.tight_layout()\n","plt.show()\n","\n","#Shows by using one-hot encoding are trading off slight AUC gain for lower classification generalization (likely due to varying sample size per ticker)\n","# Also likely due to imbalanced threhold like .7 quantile."]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
