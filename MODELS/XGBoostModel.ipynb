{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["XGBoost Model for the 3year data (includes 3 years of finance data and 3 years of news data) with **walkfoward testing**"],"metadata":{"id":"3Y5x04znAdUW"}},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"7Tj7bHChfREt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from math import sqrt\n","import numpy as np\n","\n","#load and prep data\n","df = pd.read_csv(\"Combined_3year_data.csv\")\n","df[\"date\"] = pd.to_datetime(df[\"date\"])\n","df = df.sort_values(by=[\"ticker\", \"date\"])\n","df = df.dropna()\n","\n","#define features and target\n","features = [\n","    \"average_sentiment\", \"comment_volume\", \"RealizedVol_3d\"\n","]\n","target = \"Target\"\n","\n","#store RMSEs and predictions\n","rmse_list = []\n","predictions = []\n","\n","#loop over each ticker separately\n","for ticker in df[\"ticker\"].unique():\n","    ticker_df = df[df[\"ticker\"] == ticker].copy().reset_index(drop=True)\n","\n","    min_train_size = 100  # minimum days before we start testing\n","    if len(ticker_df) <= min_train_size:\n","        continue\n","\n","    for i in range(min_train_size, len(ticker_df) - 1):\n","        train = ticker_df.iloc[:i]\n","        test = ticker_df.iloc[i:i+1]\n","\n","        X_train = train[features]\n","        y_train = train[target]\n","        X_test = test[features]\n","        y_test = test[target]\n","\n","        #model pipeline\n","        model = make_pipeline(\n","            StandardScaler(),\n","            XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42)\n","        )\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        rmse = sqrt(mean_squared_error(y_test, y_pred))\n","        rmse_list.append(rmse)\n","        predictions.append({\n","            \"date\": test[\"date\"].values[0],\n","            \"ticker\": ticker,\n","            \"true\": y_test.values[0],\n","            \"predicted\": y_pred[0]\n","        })\n","\n","#results\n","avg_rmse = np.mean(rmse_list)\n","print(\"Average RMSE across all steps:\", np.mean(rmse_list))\n","\n","# convert predictions to DataFrame for plotting\n","pred_df = pd.DataFrame(predictions)"],"metadata":{"id":"ZMyletxQgKUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","#plot prediction vs actual for a ticker\n","sample = pred_df[pred_df[\"ticker\"] == \"TSLA\"]\n","\n","plt.figure(figsize=(12, 5))\n","plt.plot(sample[\"date\"], sample[\"true\"], label=\"True Volatility\")\n","plt.plot(sample[\"date\"], sample[\"predicted\"], label=\"Predicted Volatility\")\n","plt.title(\"Walk-Forward Volatility Prediction - AAPL\")\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"l9imGG-3gsY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compare to the GARCH model\n","from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"DkYgWrlDkZFT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","garch_df = pd.read_csv(\"garch_volatility_predictions.csv\")\n","\n","#prepare GARCH prediction\n","garch_df[\"date\"] = pd.to_datetime(garch_df[\"date\"])\n","garch_df = garch_df.rename(columns={\"Rolling_GARCH_volatility %\": \"garch_predicted\"})\n","garch_df[\"garch_predicted\"] = garch_df[\"garch_predicted\"].astype(float)\n","\n","# merge and compare\n","merged = pd.merge(pred_df, garch_df, on=[\"ticker\", \"date\"])\n","merged[\"garch_predicted\"] = merged[\"garch_predicted\"].astype(float) / 100\n","garch_rmse = sqrt(mean_squared_error(merged[\"true\"], merged[\"garch_predicted\"]))\n","\n","print(f\"XGBoost RMSE: {avg_rmse:.4f}\")\n","print(f\"GARCH RMSE: {garch_rmse:.4f}\")"],"metadata":{"id":"3Zm5Vip3vNXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#subsample for performance\n","sampled = merged.sample(n=1000, random_state=42) if len(merged) > 1000 else merged.copy()\n","\n","#true vs predicted Scatter Plot\n","plt.figure(figsize=(14, 6))\n","\n","# XGBoost subplot\n","plt.subplot(1, 2, 1)\n","sns.scatterplot(data=sampled, x=\"true\", y=\"predicted\", alpha=0.5, edgecolor=None)\n","plt.plot([sampled[\"true\"].min(), sampled[\"true\"].max()],\n","         [sampled[\"true\"].min(), sampled[\"true\"].max()],\n","         color=\"red\", linestyle=\"--\")\n","plt.title(\"XGBoost: True vs Predicted Volatility\")\n","plt.xlabel(\"True Volatility\")\n","plt.ylabel(\"XGBoost Predicted\")\n","\n","# GARCH subplot\n","plt.subplot(1, 2, 2)\n","sns.scatterplot(data=sampled, x=\"true\", y=\"garch_predicted\", alpha=0.5, edgecolor=None)\n","plt.plot([sampled[\"true\"].min(), sampled[\"true\"].max()],\n","         [sampled[\"true\"].min(), sampled[\"true\"].max()],\n","         color=\"red\", linestyle=\"--\")\n","plt.title(\"GARCH: True vs Predicted Volatility\")\n","plt.xlabel(\"True Volatility\")\n","plt.ylabel(\"GARCH Predicted\")\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"XVQ-DO3SBmYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Error Analysis\n","#add residuals\n","merged[\"xgb_error\"] = merged[\"true\"] - merged[\"predicted\"]\n","merged[\"garch_error\"] = merged[\"true\"] - merged[\"garch_predicted\"]\n","\n","#histogram of residuals\n","import matplotlib.pyplot as plt\n","plt.hist(merged[\"xgb_error\"], bins=50, alpha=0.6, label=\"XGBoost\")\n","plt.hist(merged[\"garch_error\"], bins=50, alpha=0.6, label=\"GARCH\")\n","plt.title(\"Distribution of Prediction Errors\")\n","plt.xlabel(\"Prediction Error\")\n","plt.ylabel(\"Frequency\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"7GqcW5vLlrqa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["XGBoost Model for the 3year data (includes 3 years of finance data and 3 years of news data) with **80/20 train/test split**"],"metadata":{"id":"Eb3WhY2WAxTo"}},{"cell_type":"code","source":["import pandas as pd\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from math import sqrt\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#load and prep data\n","df = pd.read_csv(\"Combined_3year_data.csv\")\n","df[\"date\"] = pd.to_datetime(df[\"date\"])\n","df = df.sort_values(by=[\"ticker\", \"date\"])\n","df = df.dropna()\n","\n","#define features and target\n","features3 = [\n","    \"average_sentiment\", \"comment_volume\", \"RealizedVol_3d\"\n","]\n","target3 = \"Target\"\n","\n","#80/20 split by date\n","split_date3 = df[\"date\"].quantile(0.8)\n","train3 = df[df[\"date\"] <= split_date3]\n","test3 = df[df[\"date\"] > split_date3]\n","\n","X_train3 = train3[features3]\n","y_train3 = train3[target3]\n","X_test3 = test3[features3]\n","y_test3 = test3[target3]\n","\n","#train model\n","model3 = make_pipeline(\n","    StandardScaler(),\n","    XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42)\n",")\n","model3.fit(X_train3, y_train3)\n","\n","#predict and evaluate\n","test3 = test3.copy()\n","test3[\"predicted_target\"] = model3.predict(X_test3)\n","rmse3 = sqrt(mean_squared_error(y_test3, test3[\"predicted_target\"]))\n","print(\"Time-aware test RMSE:\", rmse3)"],"metadata":{"id":"y_T7VCV1eWPA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pick a ticker to visualize\n","ticker = \"AAPL\"\n","df_plot3 = test3[test3[\"ticker\"] == ticker].copy()\n","\n","plt.figure(figsize=(12, 6))\n","sns.lineplot(data=df_plot3, x=\"date\", y=\"Target\", label=\"Actual\", linewidth=2)\n","sns.lineplot(data=df_plot3, x=\"date\", y=\"predicted_target\", label=\"Predicted\", linewidth=2)\n","plt.title(f\"Predicted vs Actual Volatility (`Target`) for {ticker}\")\n","plt.ylabel(\"Volatility (target)\")\n","plt.xlabel(\"Date\")\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"t-fiKMp82628"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["XGBoost Model for the 1year data (includes 1 year of finance data, 1 years of news data, 1 year of reddit data) with **walkfoward testing**"],"metadata":{"id":"elsDo5x_BHdU"}},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"xdKJws836WCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from math import sqrt\n","import numpy as np\n","\n","#load and prep data\n","df1 = pd.read_csv(\"FINAL_1YEAR_DATA.csv\")\n","df1[\"date\"] = pd.to_datetime(df1[\"date\"])\n","df1 = df1.sort_values(by=[\"ticker\", \"date\"])\n","df1 = df1.dropna()\n","\n","#define features and target\n","features1yr = [\n","    \"reddit_sentiment_lag1\", \"reddit_volume_lag1\", \"RealizedVol_3d\", \"news_sentiment_lag1\", \"news_volume_lag1\"\n","]\n","target1yr = \"Target\"\n","\n","#store RMSEs and predictions\n","rmse_list1yr = []\n","predictions1yr = []\n","\n","#loop over each ticker separately (you could expand to global walk-forward if needed)\n","for ticker in df[\"ticker\"].unique():\n","    ticker_df1yr = df1[df1[\"ticker\"] == ticker].copy().reset_index(drop=True)\n","\n","    min_train_size = 100  # minimum days before we start testing\n","    if len(ticker_df1yr) <= min_train_size:\n","        continue\n","\n","    for i in range(min_train_size, len(ticker_df1yr) - 1):\n","        train1yr = ticker_df1yr.iloc[:i]\n","        test1yr = ticker_df1yr.iloc[i:i+1]\n","\n","        X_train1yr = train1yr[features1yr]\n","        y_train1yr = train1yr[target1yr]\n","        X_test1yr = test1yr[features1yr]\n","        y_test1yr = test1yr[target1yr]\n","\n","        #model pipeline\n","        model1yr = make_pipeline(\n","            StandardScaler(),\n","            XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42)\n","        )\n","        model1yr.fit(X_train1yr, y_train1yr)\n","        y_pred1yr = model1yr.predict(X_test1yr)\n","\n","        rmse1yr = sqrt(mean_squared_error(y_test1yr, y_pred1yr))\n","        rmse_list1yr.append(rmse1yr)\n","        predictions1yr.append({\n","            \"date\": test1yr[\"date\"].values[0],\n","            \"ticker\": ticker,\n","            \"true\": y_test1yr.values[0],\n","            \"predicted\": y_pred1yr[0]\n","        })\n","\n","#results\n","print(\"Average RMSE across all steps:\", np.mean(rmse_list1yr))\n","\n","# convert predictions to DataFrame for plotting\n","pred_df1yr = pd.DataFrame(predictions1yr)"],"metadata":{"id":"Se2xSs0d6fTT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","#plot prediction vs actual for a ticker\n","sample1 = pred_df1yr[pred_df1yr[\"ticker\"] == \"AAPL\"]\n","\n","plt.figure(figsize=(12, 5))\n","plt.plot(sample1[\"date\"], sample1[\"true\"], label=\"True Volatility\")\n","plt.plot(sample1[\"date\"], sample1[\"predicted\"], label=\"Predicted Volatility\")\n","plt.title(\"Walk-Forward Volatility Prediction - AAPL\")\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"3SctuWii_DNo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["XGBoost Model for the 1year data (includes 1 year of finance data, 1 years of news data, 1 year of reddit data) with **80/20 train/test split**"],"metadata":{"id":"KxR90s5VBRVm"}},{"cell_type":"code","source":["#define features and target\n","features1 = [\n","        \"reddit_sentiment_lag1\", \"reddit_volume_lag1\", \"RealizedVol_3d\", \"news_sentiment_lag1\", \"news_volume_lag1\"\n","]\n","target1 = \"Target\"\n","\n","#80/20 split by date\n","split_date1 = df1[\"date\"].quantile(0.8)\n","train1 = df1[df1[\"date\"] <= split_date1]\n","test1 = df1[df1[\"date\"] > split_date1]\n","\n","X_train1 = train1[features1]\n","y_train1 = train1[target1]\n","X_test1 = test1[features1]\n","y_test1 = test1[target1]\n","\n","#train model\n","model1 = make_pipeline(\n","    StandardScaler(),\n","    XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42)\n",")\n","model1.fit(X_train1, y_train1)\n","\n","#predict and evaluate\n","test1 = test1.copy()\n","test1[\"predicted_target\"] = model1.predict(X_test1)\n","rmse1 = sqrt(mean_squared_error(y_test1, test1[\"predicted_target\"]))\n","print(\"Time-aware test RMSE:\", rmse1)"],"metadata":{"id":"fanxmx5y9QEB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pick a ticker to visualize\n","ticker = \"AAPL\"\n","df_plot1 = test1[test1[\"ticker\"] == ticker].copy()\n","\n","plt.figure(figsize=(12, 6))\n","sns.lineplot(data=df_plot1, x=\"date\", y=\"Target\", label=\"Actual\", linewidth=2)\n","sns.lineplot(data=df_plot1, x=\"date\", y=\"predicted_target\", label=\"Predicted\", linewidth=2)\n","plt.title(f\"Predicted vs Actual Volatility (`Target`) for {ticker}\")\n","plt.ylabel(\"Volatility (target)\")\n","plt.xlabel(\"Date\")\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"_CM6DtxE-PFS"},"execution_count":null,"outputs":[]}]}