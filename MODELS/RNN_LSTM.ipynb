{"cells":[{"cell_type":"code","execution_count":null,"id":"1543dae5","metadata":{"id":"1543dae5"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import explained_variance_score\n",""]},{"cell_type":"code","execution_count":null,"id":"b958d79d","metadata":{"id":"b958d79d"},"outputs":[],"source":["df = pd.read_csv('Combined_3year_data.csv')\n","\n","df['date'] = pd.to_datetime(df['date'])\n","print(df.groupby('date').head())\n","df['combined_sentiment_lag1'] = df.groupby('ticker')['average_sentiment'].shift(1)\n","df['combined_volume_lag1'] = df.groupby('ticker')['comment_volume'].shift(1)\n","df['sentiment_missing'] = df['combined_sentiment_lag1'].isna().astype(int)\n","\n","df['Target_lag1'] = df.groupby('ticker')['Target'].shift(1)\n","df['Target_lag2'] = df.groupby('ticker')['Target'].shift(2)\n","df['Target_smooth'] = df.groupby('ticker')['Target'].transform(lambda x: x.rolling(3).mean())\n","\n","df.dropna(subset=['Target_lag1', 'Target_lag2', 'Target_smooth', 'combined_sentiment_lag1'], inplace=True)\n","\n","features = [\n","\n","    'RealizedVol_3d',\n","    'combined_sentiment_lag1',\n","    'combined_volume_lag1',\n","    'sentiment_missing',\n","    'Target_lag1',\n","    'Target_lag2'\n","\n","]\n","\n","target = 'Target_smooth'\n","\n","df.fillna(method='ffill', inplace=True)\n","\n","scaler_X = MinMaxScaler()\n","df[features] = scaler_X.fit_transform(df[features])\n","\n","scaler_y = StandardScaler()\n","df[target] = scaler_y.fit_transform(df[[target]])"]},{"cell_type":"code","execution_count":null,"id":"e9edb5e1","metadata":{"id":"e9edb5e1"},"outputs":[],"source":["def create_sequences(data, seq_len, features, target):\n","    X, y = [], []\n","    for _, group in data.groupby('ticker'):\n","        group = group.sort_values('date')\n","        for i in range(len(group) - seq_len):\n","            seq_x = group[features].iloc[i:i+seq_len].values\n","            seq_y = group[target].iloc[i+seq_len]\n","            X.append(seq_x)\n","            y.append(seq_y)\n","    return np.array(X), np.array(y)\n","\n","sequence_length = 10\n","X, y = create_sequences(df, sequence_length, features, target)\n"]},{"cell_type":"code","execution_count":null,"id":"dd458b0b","metadata":{"id":"dd458b0b"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n","\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"]},{"cell_type":"code","execution_count":null,"id":"baf8c9f0","metadata":{"id":"baf8c9f0"},"outputs":[],"source":["class VolatilityLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=128, num_layers=3, output_dim=1):\n","        super(VolatilityLSTM, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","model = VolatilityLSTM(input_dim=len(features))\n"]},{"cell_type":"code","execution_count":null,"id":"0d34fc17","metadata":{"id":"0d34fc17"},"outputs":[],"source":["criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","epochs = 300\n","for epoch in range(epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    output = model(X_train)\n","    loss = criterion(output, y_train)\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"631742f1","metadata":{"id":"631742f1"},"outputs":[],"source":["model.eval()\n","with torch.no_grad():\n","    y_pred_scaled = model(X_test).numpy()\n","    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n","    y_true = scaler_y.inverse_transform(y_test.numpy())\n","\n","mse = mean_squared_error(y_true, y_pred)\n","r2 = r2_score(y_true, y_pred)\n","mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","direction_true = np.sign(y_true[1:] - y_true[:-1])\n","direction_pred = np.sign(y_pred[1:] - y_pred[:-1])\n","directional_acc = np.mean(direction_true == direction_pred)\n","\n","print(f\"\\n--- EVALUATION METRICS ---\")\n","print(f\"R² Score         : {r2:.4f}\")\n","print(f\"MAPE             : {mape:.2f}%\")\n","print(f\"Directional Acc. : {directional_acc:.4f}\")\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(y_true, label='True Volatility', linewidth=2)\n","plt.plot(y_pred, label='Predicted Volatility', linewidth=2)\n","plt.title(\"LSTM Volatility Prediction Over Time (Combined Sentiment)\", fontsize=14)\n","plt.xlabel(\"Time\")\n","plt.ylabel(\"Volatility\")\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(y_true, y_pred, alpha=0.5)\n","plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', linewidth=2)\n","plt.title(\"LSTM: True vs Predicted Volatility\", fontsize=14)\n","plt.xlabel(\"True Volatility\")\n","plt.ylabel(\"Predicted Volatility\")\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"fc61c53f","metadata":{"id":"fc61c53f"},"outputs":[],"source":["mse = mean_squared_error(y_true, y_pred)\n","rmse = np.sqrt(mse)\n","print(f\"Test RMSE        : {rmse:.4f}\")\n","print(f\"R² Score         : {r2:.4f}\")\n","print(f\"MAPE             : {mape:.2f}%\")\n","print(f\"Directional Acc. : {directional_acc:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"6cd38efd","metadata":{"id":"6cd38efd"},"outputs":[],"source":["baseline_preds = model(X_test).detach().numpy()\n","baseline_rmse = np.sqrt(mean_squared_error(y_test,baseline_preds))\n","\n","importances = {}\n","for i, feat in enumerate(features):\n","    X_test_permuted = X_test.clone()\n","    idx = torch.randperm(X_test.shape[0])\n","    X_test_permuted[:, :, i] = X_test_permuted[idx,:,i]\n","    y_perm = model(X_test_permuted).detach().numpy()\n","    rmse = np.sqrt(mean_squared_error(y_test, y_perm))\n","    importances[feat] = rmse - baseline_rmse\n","\n","plt.figure(figsize=(10,5))\n","plt.bar(importances.keys(),importances.values())\n","plt.title(\"Permutation Feature Importance (RMSE Increase)\")\n","plt.ylabel(\"Δ RMSE\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"7ca1836c","metadata":{"id":"7ca1836c"},"outputs":[],"source":["mae = mean_absolute_error(y_true, y_pred)\n","print(f\"MAE              : {mae:.4f}\")\n","baseline_mae = mean_absolute_error(y_test, baseline_preds)\n","mae_importances = {}\n","\n","for i, feat in enumerate(features):\n","    X_test_permuted = X_test.clone()\n","    idx = torch.randperm(X_test.shape[0])\n","    X_test_permuted[:, :, i] = X_test_permuted[idx, :, i]\n","    y_perm = model(X_test_permuted).detach().numpy()\n","    perm_mae = mean_absolute_error(y_test, y_perm)\n","    mae_importances[feat] = perm_mae - baseline_mae\n","\n","plt.figure(figsize=(10, 5))\n","plt.bar(mae_importances.keys(), mae_importances.values(), color='slateblue')\n","plt.title(\"Permutation Feature Importance (MAE Increase)\")\n","plt.ylabel(\"MAE\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"85ba6f16","metadata":{"id":"85ba6f16"},"outputs":[],"source":["def smape(y_true, y_pred):\n","    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n","\n","smape_score = smape(y_true, y_pred)\n","print(f\"SMAPE (%)        : {smape_score:.2f}%\")\n","\n","explained_var = explained_variance_score(y_true, y_pred)\n","print(f\"Explained Var.   : {explained_var:.4f}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
