{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment\n",
        "!pip install pandas_market_calendars"
      ],
      "metadata": {
        "id": "VOncCVZ4gQv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "nLpyLNCwzZl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZbONPaQxRh9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import pandas_market_calendars as mcal\n",
        "\n",
        "#load data\n",
        "df = pd.read_csv(\"HackerNews.csv\")\n",
        "#df = pd.read_csv('/content/drive/MyDrive/NewsDataCX4240/HackerNews.csv')\n",
        "\n",
        "#clean text function\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"&gt;|&lt;|&amp;|&#x27;|<p>\", \"\", text)\n",
        "    return text.lower()\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "#convert timestamp and extract date\n",
        "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "df[\"date\"] = df[\"timestamp\"].dt.date\n",
        "\n",
        "#get NYSE trading days\n",
        "nyse = mcal.get_calendar(\"NYSE\")\n",
        "trading_days = nyse.valid_days(\n",
        "    start_date=str(df[\"date\"].min()),\n",
        "    end_date=str(df[\"date\"].max())\n",
        ").date\n",
        "\n",
        "#filter to trading days only\n",
        "df = df[df[\"date\"].isin(trading_days)]\n",
        "\n",
        "\n",
        "#define tickers and common names\n",
        "ticker_to_keywords = {\n",
        "    \"AAPL\": [\"aapl\", \"apple\"],\n",
        "    \"MSFT\": [\"msft\", \"microsoft\"],\n",
        "    \"NVDA\": [\"nvda\", \"nvidia\"],\n",
        "    \"AMZN\": [\"amzn\", \"amazon\"],\n",
        "    \"META\": [\"meta\", \"facebook\", \"meta platforms\"],\n",
        "    \"BRK.B\": [\"brk.b\", \"berkshire\", \"buffett\", \"berkshire hathaway\"],\n",
        "    \"GOOGL\": [\"googl\", \"alphabet\", \"google\"],\n",
        "    \"GOOG\": [\"goog\", \"alphabet\", \"google\"],\n",
        "    \"TSLA\": [\"tsla\", \"tesla\", \"elon\"],\n",
        "    \"JPM\": [\"jpm\", \"jpmorgan\", \"jp morgan\"],\n",
        "    \"UNH\": [\"unh\", \"unitedhealth\"],\n",
        "    \"ACHR\": [\"archer\", \"archer aviation\", \"achr\"],\n",
        "    \"COST\": [\"cost\", \"costco\"],\n",
        "    \"NFLX\": [\"nflx\", \"netflix\"],\n",
        "    \"WMT\": [\"wmt\", \"walmart\"],\n",
        "    \"PG\": [\"pg\", \"procter\", \"procter and gamble\"],\n",
        "    \"JNJ\": [\"jnj\", \"johnson\", \"johnson & johnson\"],\n",
        "    \"HD\": [\"hd\", \"home depot\"],\n",
        "    \"KO\": [\"ko\", \"coca cola\", \"coke\"],\n",
        "    \"CRM\": [\"crm\", \"salesforce\"],\n",
        "    \"PM\": [\"pm\", \"philip morris\"],\n",
        "    \"CVX\": [\"cvx\", \"chevron\"],\n",
        "    \"CRWD\": [\"crwd\", \"crowdstrike\"],\n",
        "    \"MCD\": [\"mcd\", \"mcdonalds\", \"mickey d's\"],\n",
        "    \"ORCL\": [\"orcl\", \"oracle\"],\n",
        "    \"ABT\": [\"abt\", \"abbott\"],\n",
        "    \"IBM\": [\"ibm\", \"big blue\"],\n",
        "    \"WFC\": [\"wfc\", \"wells fargo\"],\n",
        "    \"PEP\": [\"pep\", \"pepsi\", \"pepsico\"],\n",
        "    \"MRK\": [\"mrk\", \"merck\"],\n",
        "    \"PLTR\": [\"pltr\", \"palantir\"],\n",
        "    \"VZ\": [\"vz\", \"verizon\"],\n",
        "    \"ACN\": [\"acn\", \"accenture\"],\n",
        "    \"ISRG\": [\"isrg\", \"intuitive surgical\"],\n",
        "    \"RTX\": [\"rtx\", \"raytheon\", \"rtx corp\"],\n",
        "    \"TMO\": [\"tmo\", \"thermo fisher\"],\n",
        "    \"INTU\": [\"intu\", \"intuit\"],\n",
        "    \"PGR\": [\"pgr\", \"progressive\"],\n",
        "}\n",
        "\n",
        "#match ticker using keywords\n",
        "def find_matching_ticker(text):\n",
        "    for ticker, keywords in ticker_to_keywords.items():\n",
        "        for keyword in keywords:\n",
        "            if re.search(rf\"\\b{re.escape(keyword)}\\b\", text):\n",
        "                return ticker\n",
        "    return None\n",
        "\n",
        "# apply ticker matching\n",
        "df[\"matched_ticker\"] = df[\"clean_text\"].apply(find_matching_ticker)\n",
        "\n",
        "# view matched rows\n",
        "matched_df = df[df[\"matched_ticker\"].notnull()][[\"timestamp\", \"clean_text\", \"matched_ticker\"]]\n",
        "print(matched_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#number of comments for each ticker\n",
        "ticker_counts_df = df[\"matched_ticker\"].value_counts().reset_index()\n",
        "ticker_counts_df.columns = [\"ticker\", \"count\"]\n",
        "print(ticker_counts_df)"
      ],
      "metadata": {
        "id": "xxYNBP1Fc672"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of mentions per ticker per day\n",
        "# Convert timestamp to datetime and extract date\n",
        "df[\"date\"] = pd.to_datetime(df[\"timestamp\"]).dt.date\n",
        "\n",
        "# Drop rows with no matched ticker\n",
        "filtered_df = df[df[\"matched_ticker\"].notnull()]\n",
        "\n",
        "#group by date and ticker, then count\n",
        "mentions_per_day = filtered_df.groupby([\"date\", \"matched_ticker\"]).size().reset_index(name=\"count\")\n",
        "\n",
        "# sort for easier viewing\n",
        "mentions_per_day = mentions_per_day.sort_values(by=[\"date\", \"count\"], ascending=[False, False])\n",
        "\n",
        "print(mentions_per_day.head(34))"
      ],
      "metadata": {
        "id": "G2TqRHZKdSIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#total number of ticker mentions per day\n",
        "df[\"date\"] = pd.to_datetime(df[\"timestamp\"]).dt.date\n",
        "\n",
        "#filter only rows where a ticker was matched\n",
        "filtered_df = df[df[\"matched_ticker\"].notnull()]\n",
        "\n",
        "#count ticker mentions per day\n",
        "daily_mentions = filtered_df.groupby(\"date\").size().reset_index(name=\"total_mentions\")\n",
        "\n",
        "print(daily_mentions)\n"
      ],
      "metadata": {
        "id": "UXzIhHA3eell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment analysis using VADER\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return scores[\"compound\"]  #compound score is overall sentiment\n",
        "\n",
        "df[\"sentiment\"] = df[\"clean_text\"].apply(get_sentiment)\n",
        "\n",
        "#group by date and calculate average sentiment\n",
        "df[\"date\"] = pd.to_datetime(df[\"timestamp\"]).dt.date\n",
        "\n",
        "#filter only rows where a ticker was matched\n",
        "filtered_df = df[df[\"matched_ticker\"].notnull()]\n",
        "\n",
        "#calculate sentiment per day\n",
        "daily_sentiment = filtered_df.groupby(\"date\")[\"sentiment\"].mean().reset_index()\n",
        "daily_sentiment.columns = [\"date\", \"average_sentiment\"]\n",
        "\n",
        "#average sentiment per ticker per day\n",
        "sentiment_by_ticker_day = filtered_df.groupby([\"date\", \"matched_ticker\"])[\"sentiment\"].mean().reset_index()\n",
        "sentiment_by_ticker_day.columns = [\"date\", \"ticker\", \"average_sentiment\"]\n",
        "\n",
        "#results\n",
        "print(\"Average Sentiment per Day:\")\n",
        "print(daily_sentiment.head(10))\n",
        "\n",
        "print(\"\\nAverage Sentiment per Ticker per Day:\")\n",
        "print(sentiment_by_ticker_day.head(34))"
      ],
      "metadata": {
        "id": "UEmRPDExe0p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas_market_calendars as mcal\n",
        "\n",
        "#csv for 3 year data\n",
        "#calculate comments per day per ticker\n",
        "ticker_volume_by_day = filtered_df.groupby([\"date\", \"matched_ticker\"]).size().reset_index(name=\"comment_volume\")\n",
        "\n",
        "#calculate average sentiment per day per ticker\n",
        "ticker_sentiment_by_day = filtered_df.groupby([\"date\", \"matched_ticker\"])[\"sentiment\"].mean().reset_index(name=\"average_sentiment\")\n",
        "\n",
        "# merge volume and sentiment data on date and ticker\n",
        "final_data = pd.merge(\n",
        "    ticker_volume_by_day,\n",
        "    ticker_sentiment_by_day,\n",
        "    on=[\"date\", \"matched_ticker\"],\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "#rename for clarity\n",
        "final_data = final_data.rename(columns={\"matched_ticker\": \"ticker\"})\n",
        "\n",
        "# define date range and tickers\n",
        "yr3start_date = \"2022-01-10\"\n",
        "yr3end_date = \"2025-04-11\"\n",
        "\n",
        "yr3selected_tickers = [\n",
        "    'PG', 'MCD', 'TSLA', 'JPM', 'GOOGL', 'AAPL', 'COST', 'KO', 'NFLX', 'PM', 'META',\n",
        "    'NVDA', 'BRK.B', 'WMT', 'AMZN', 'HD', 'GOOG', 'MSFT', 'PLTR', 'INTU', 'JNJ',\n",
        "    'ORCL', 'PEP', 'RTX', 'VZ', 'IBM', 'CRM', 'PGR'\n",
        "]\n",
        "\n",
        "# ensure datetime format\n",
        "final_data[\"date\"] = pd.to_datetime(final_data[\"date\"]).dt.normalize()\n",
        "\n",
        "#filter to desired tickers and date range\n",
        "filtered_data3 = final_data[\n",
        "    (final_data[\"ticker\"].isin(yr3selected_tickers)) &\n",
        "    (final_data[\"date\"] >= pd.to_datetime(yr3start_date)) &\n",
        "    (final_data[\"date\"] <= pd.to_datetime(yr3end_date))\n",
        "]\n",
        "\n",
        "#Fill in missing NYSE trading days\n",
        "nyse = mcal.get_calendar(\"NYSE\")\n",
        "nyse_days = nyse.valid_days(start_date=yr3start_date, end_date=yr3end_date).tz_localize(None)\n",
        "nyse_days = pd.to_datetime(nyse_days).normalize()\n",
        "\n",
        "#create full date-ticker index\n",
        "full_index = pd.MultiIndex.from_product(\n",
        "    [nyse_days, yr3selected_tickers],\n",
        "    names=[\"date\", \"ticker\"]\n",
        ")\n",
        "\n",
        "# reindex to complete grid\n",
        "filled_data = filtered_data3.set_index([\"date\", \"ticker\"]).reindex(full_index).reset_index()\n",
        "\n",
        "# fill missing values with 0\n",
        "filled_data[\"comment_volume\"] = filled_data[\"comment_volume\"].fillna(0)\n",
        "filled_data[\"average_sentiment\"] = filled_data[\"average_sentiment\"].fillna(0)\n",
        "\n",
        "# Sort and Save\n",
        "filled_data = filled_data.sort_values(by=[\"date\", \"ticker\"])\n",
        "filled_data.to_csv(\"year3news.csv\", index=False)\n",
        "\n",
        "print(filled_data.head(20))\n",
        "\n",
        "# uncomment below to download csv\n",
        "#from google.colab import files\n",
        "#files.download(\"year3news.csv\")\n"
      ],
      "metadata": {
        "id": "_URoCtTxq69m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#csv for 1 Year data\n",
        "selected_tickers = [\n",
        "    \"NVDA\",  # Nvidia\n",
        "    \"AAPL\",  # Apple\n",
        "    \"COST\",  # Costco Wholesale Corp.\n",
        "    \"TSLA\",  # Tesla, Inc.\n",
        "    \"GOOGL\", # Alphabet Inc. Class A\n",
        "    \"PM\",    # Philip Morris International Inc.\n",
        "    \"MSFT\",  # Microsoft\n",
        "    \"META\",  # Meta Platforms, Inc. Class A\n",
        "    \"KO\",    # Coca-Cola Company\n",
        "    \"AMZN\",  # Amazon.com Inc.\n",
        "    \"PG\",    # Procter & Gamble Company\n",
        "    \"BRK.B\", # Berkshire Hathaway Class B\n",
        "    \"HD\",    # Home Depot, Inc.\n",
        "    \"GOOG\",  # Alphabet Inc. Class C\n",
        "    \"PLTR\"   # Palantir\n",
        "]\n",
        "\n",
        "start_date = \"2024-02-02\"\n",
        "end_date = \"2025-04-11\"\n",
        "\n",
        "\n",
        "# Filter the data by ticker and date range\n",
        "filtered_data = final_data[\n",
        "    (final_data['ticker'].isin(selected_tickers)) &\n",
        "    (final_data['date'] >= pd.to_datetime(start_date)) &\n",
        "    (final_data['date'] <= pd.to_datetime(end_date))\n",
        "]\n",
        "\n",
        "# fill in missing NYSE trading days\n",
        "nyse_days1 = nyse.valid_days(start_date=start_date, end_date=end_date).tz_localize(None)\n",
        "nyse_days1 = pd.to_datetime(nyse_days1).normalize()\n",
        "\n",
        "# Create full date-ticker index\n",
        "full_index1 = pd.MultiIndex.from_product(\n",
        "    [nyse_days1, selected_tickers],\n",
        "    names=[\"date\", \"ticker\"]\n",
        ")\n",
        "\n",
        "# Reindex to complete grid\n",
        "filled_data1 = filtered_data.set_index([\"date\", \"ticker\"]).reindex(full_index1).reset_index()\n",
        "\n",
        "# Fill missing values with 0\n",
        "filled_data1[\"comment_volume\"] = filled_data1[\"comment_volume\"].fillna(0)\n",
        "filled_data1[\"average_sentiment\"] = filled_data1[\"average_sentiment\"].fillna(0)\n",
        "\n",
        "#sort and save\n",
        "filled_data1 = filled_data1.sort_values(by=[\"date\", \"ticker\"])\n",
        "\n",
        "# Uncomment below to save csv\n",
        "#from google.colab import files\n",
        "#filled_data1.to_csv(\"News1year.csv\", index=False)\n",
        "\n",
        "print(filled_data.head(20))"
      ],
      "metadata": {
        "id": "Ne7qZn5cut3Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}