{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggKAYul-Xkyi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6J394QBjm6-"
      },
      "outputs": [],
      "source": [
        "#Install packages\n",
        "!pip install arch\n",
        "!pip install praw\n",
        "!pip install nltk\n",
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npLlO2ykXm42"
      },
      "outputs": [],
      "source": [
        "#PULL PRICE DATA FOR SELECTED STOCKS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from arch import arch_model\n",
        "import yfinance as yf\n",
        "\n",
        "#Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "tickers = [\n",
        "    'PG', 'MCD', 'TSLA', 'JPM', 'GOOGL', 'AAPL', 'COST', 'KO', 'NFLX', 'PM', 'META',\n",
        " 'NVDA', 'BRK-B', 'WMT', 'AMZN', 'HD', 'CRWD', 'UNH', 'GOOG', 'MSFT', 'PLTR', 'INTU',\n",
        " 'ABT', 'JNJ', 'ORCL', 'ACHR', 'TMO', 'PEP', 'RTX', 'VZ', 'CVX', 'WFC', 'IBM', 'CRM',\n",
        " 'ISRG', 'MRK', 'PGR', 'ACN'\n",
        "]\n",
        "\n",
        "start_date = '2022-01-01'\n",
        "end_date =  '2025-04-11'\n",
        "#marketDays = 504\n",
        "price_data = yf.download(tickers,start=start_date, end= end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n89LMQ_EgZWF"
      },
      "outputs": [],
      "source": [
        "# Check for missing tickers (Survivorship Bias)\n",
        "missing_tickers = [t for t in tickers if t not in price_data.columns.get_level_values(1)]\n",
        "if missing_tickers:\n",
        "    print(\"Warning: The following stocks are missing (possibly delisted):\", missing_tickers)\n",
        "else:\n",
        "    print(\"No missing stocks detected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh8l6wy6Xh6T"
      },
      "outputs": [],
      "source": [
        "#Reformat data into a clean pd datafram with one row per stock per day.\n",
        "changed_df = []\n",
        "for ticker in tickers:\n",
        "    ticker_df = price_data.xs(ticker, level=\"Ticker\", axis=1).copy()\n",
        "    ticker_df.loc[:,'ticker'] = ticker\n",
        "    ticker_df.loc[:,'date'] = ticker_df.index\n",
        "    changed_df.append(ticker_df.reset_index(drop=True))\n",
        "\n",
        "combined_df = pd.concat(changed_df).reset_index(drop=True)\n",
        "#print(combined_df.columns.tolist())\n",
        "combined_df = combined_df.drop(columns=['Open','High','Low','Volume'])\n",
        "combined_df.columns.name = None\n",
        "print(\"Columns: \")\n",
        "print(combined_df.columns.tolist())\n",
        "print(\"Data head: \")\n",
        "print(combined_df.head())\n",
        "print(\"Data Shape: \")\n",
        "print(combined_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAyPDNUqXvGs"
      },
      "outputs": [],
      "source": [
        "#Checking the data\n",
        "combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
        "full_dates = pd.date_range(start=combined_df['date'].min(),\n",
        "                           end=combined_df['date'].max(),\n",
        "                           freq= 'B')\n",
        "pivot_df = combined_df.pivot(index='date', columns='ticker', values='Close')\n",
        "complete_df = pivot_df.reindex(full_dates)\n",
        "\n",
        "missing_counts = complete_df.isna().sum()\n",
        "print(\"Missing days by ticker:\")\n",
        "print(missing_counts)\n",
        "\n",
        "#All stocks were missing the same # days (19). \"B\" function was not accounting for holidays like July 4th, memorial day, etc...\n",
        "missing_msft = complete_df[complete_df['MSFT'].isna()]\n",
        "print(\"Missing dates for MSFT:\")\n",
        "print(missing_msft.index.tolist())\n",
        "\n",
        "#Checked, math works out (19 * 15 = 285) and (7815 - 285 = 7530)\n",
        "#print(complete_df.size)\n",
        "#complete_df = complete_df.dropna(how='all')\n",
        "#print(complete_df.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save yahoo finance data as csv file\n",
        "df_combined.to_csv('Final_yfinance.csv')"
      ],
      "metadata": {
        "id": "XIQlaxMgoA1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f00b3kP1X0Bt"
      },
      "outputs": [],
      "source": [
        "#DATA VALIDATION/CLEANING CONTINUED\n",
        "missing_rows = combined_df[combined_df.isnull().any(axis=1)]\n",
        "print(missing_rows) #returned empty dataframe\n",
        "\n",
        "# Check for overall missing values\n",
        "print(\"Null Values per Column: \")\n",
        "print(combined_df.isnull().sum())\n",
        "\n",
        "combined_df = combined_df[['date','ticker','Close']]\n",
        "print(combined_df.head())\n",
        "print(combined_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdMfT_-2qf6p"
      },
      "outputs": [],
      "source": [
        "# Stock Split Check (Detect Large Jumps in Close Price)\n",
        "price_change = combined_df.groupby('ticker')['Close'].pct_change()\n",
        "split_suspects = combined_df[np.abs(price_change) > 0.5]  # More than 50% change\n",
        "if not split_suspects.empty:\n",
        "    print(\"Possible stock splits detected:\")\n",
        "    print(split_suspects[['date', 'ticker', 'Close']].head(10))\n",
        "# Adjust for splits\n",
        "\n",
        "# Check for Stocks with Excessive Missing Days\n",
        "expected_days = combined_df['date'].nunique()\n",
        "ticker_days = combined_df.groupby('ticker')['date'].nunique()\n",
        "missing_days = expected_days - ticker_days\n",
        "\n",
        "print(\"Stocks with missing trading days:\")\n",
        "print(missing_days[missing_days > 0])\n",
        "\n",
        "# Drop stocks with excessive missing data (>20% missing)\n",
        "threshold = 0.2 * expected_days\n",
        "dropped_stocks = missing_days[missing_days > threshold].index.tolist()\n",
        "combined_df = combined_df[~combined_df['ticker'].isin(dropped_stocks)]\n",
        "print(f\"Dropped stocks due to missing data: {dropped_stocks}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HcldtsAX2a4"
      },
      "outputs": [],
      "source": [
        "#CREATE REALIZED VOLATILITY, LAG RETURNS, OTHER COMPUTED FEATURES\n",
        "combined_df = combined_df.sort_values(by=['date', 'ticker'])\n",
        "\n",
        "combined_df['Return'] = combined_df.groupby('ticker')['Close'].pct_change()\n",
        "combined_df['Return_lag1'] = combined_df.groupby('ticker')['Return'].shift(1)\n",
        "combined_df['Return_lag2'] = combined_df.groupby('ticker')['Return'].shift(2)\n",
        "\n",
        "combined_df['RealizedVol_3d'] = combined_df.groupby('ticker')['Return'].transform(lambda x: x.rolling(window=3).std().shift(1))\n",
        "combined_df['Volatility_lag1'] = combined_df.groupby('ticker')['RealizedVol_3d'].shift(1)\n",
        "combined_df['Volatility_lag2'] = combined_df.groupby('ticker')['RealizedVol_3d'].shift(2)\n",
        "\n",
        "combined_df['Target'] = combined_df.groupby('ticker')['RealizedVol_3d'].shift(-1)\n",
        "combined_df = combined_df.dropna(subset=['Return', 'RealizedVol_3d', 'Return_lag1', 'Volatility_lag1', 'Target'])\n",
        "\n",
        "print(combined_df.shape)\n",
        "print(combined_df[['date', 'ticker', 'RealizedVol_3d', 'Target']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FINANCE FOR 3 YEAR DATA\n",
        "from google.colab import files\n",
        "df_combined1 = combined_df.drop(columns=['Return_lag1', 'Return', 'Close', 'Return_lag2', 'Volatility_lag1', 'Volatility_lag2'])\n",
        "df_combined1.to_csv('Final_yfinance.csv')\n",
        "files.download('Final_yfinance.csv')"
      ],
      "metadata": {
        "id": "qM-lkCA8oL2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u1x3oTcrhNk"
      },
      "outputs": [],
      "source": [
        "# Outlier Detection & Handling (Winsorizing Extreme Returns)\n",
        "z_scores = combined_df.groupby('ticker')['Return'].transform(lambda x: (x - x.mean()) / x.std())\n",
        "outliers = combined_df[np.abs(z_scores) > 5]\n",
        "print(f\"Extreme return outliers found: {outliers.shape[0]} rows\")\n",
        "combined_df['Return'] = combined_df['Return'].clip(lower=-0.1, upper=0.1)  # Cap returns at Â±10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sbfPkPKtfAd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from arch import arch_model\n",
        "\n",
        "#define the start and end dates\n",
        "start_date = '2022-01-01'\n",
        "end_date = '2025-04-11'\n",
        "\n",
        "#load the data (replace 'combined_df' with your actual data)\n",
        "combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
        "\n",
        "#filter the data to only include the date range you want\n",
        "df_filtered = combined_df[(combined_df['date'] >= start_date) & (combined_df['date'] <= end_date)]\n",
        "\n",
        "#initialize the list to store the results\n",
        "garch_volatility = []\n",
        "\n",
        "#define the window size for the rolling period\n",
        "window = 3\n",
        "\n",
        "#iterate over each ticker and its data within the date range\n",
        "for ticker in df_filtered['ticker'].unique():\n",
        "    ticker_data = df_filtered[df_filtered['ticker'] == ticker].copy()\n",
        "\n",
        "    for target_date in ticker_data['date'].unique():  # Iterate over each unique date\n",
        "        #filter data up to the target date (window-based approach)\n",
        "        data_up_to_target = ticker_data[ticker_data['date'] <= target_date]\n",
        "\n",
        "        if len(data_up_to_target) <= window:\n",
        "            print(f\"Not enough data for {ticker} on {target_date}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        #calculate returns for the window period\n",
        "        returns = data_up_to_target['Return'].dropna() * 100  # Daily returns (percentage)\n",
        "\n",
        "        #fit GARCH model on this window of data\n",
        "        model = arch_model(returns, vol='GARCH', p=3, q=3)\n",
        "        model_fit = model.fit(disp='off')\n",
        "\n",
        "        #forecast volatility for the next period\n",
        "        forecast = model_fit.forecast(start=len(returns)-1, horizon=1)\n",
        "        volatility = forecast.variance.iloc[-1, 0] ** 0.5  # Convert to standard deviation (volatility)\n",
        "\n",
        "        #append the result for each date\n",
        "        garch_volatility.append({\n",
        "            'ticker': ticker,\n",
        "            'date': target_date,\n",
        "            'Rolling_GARCH_volatility %': volatility\n",
        "        })\n",
        "\n",
        "#convert the results to a DataFrame\n",
        "garch_volatility_df = pd.DataFrame(garch_volatility)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the results to a CSV file\n",
        "from google.colab import files\n",
        "\n",
        "garch_volatility_df.to_csv('garch_volatility_predictions.csv', index=False)\n",
        "files.download('garch_volatility_predictions.csv')\n",
        "\n",
        "print(\"GARCH volatility predictions saved to 'garch_volatility_predictions.csv'.\")"
      ],
      "metadata": {
        "id": "Vx-Y0DIygWUC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
