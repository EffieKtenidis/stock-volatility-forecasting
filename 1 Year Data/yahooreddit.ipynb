{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6c_45NBPEnhL"},"outputs":[],"source":["import pandas as pd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJ29YfLvEnhP"},"outputs":[],"source":["# 1) Load data\n","df_stock  = pd.read_csv(\"yfinance_final.csv\", parse_dates=[\"date\"])\n","df_reddit = pd.read_csv(\"clean_reddit_data.csv\", parse_dates=[\"date\"])\n","# Ensure date column is datetime\n","df_stock['date'] = pd.to_datetime(df_stock['date'])\n","df_reddit['date'] = pd.to_datetime(df_reddit['date'])\n","\n","# Uppercase and replace dots with dashes (like yfinance style)\n","df_stock[\"ticker\"] = df_stock[\"ticker\"].str.upper().str.replace('.', '-', regex=False)\n","df_reddit[\"ticker\"] = df_reddit[\"ticker\"].str.upper().str.replace('.', '-', regex=False)\n","\n","# 2) Define date range and filter both DataFrames\n","start, end = \"2024-02-02\", \"2025-04-08\"\n","mask_stock = (df_stock['date'] >= start) & (df_stock['date'] <= end)\n","mask_reddit = (df_reddit['date'] >= start) & (df_reddit['date'] <= end)\n","\n","\n","df_stock = df_stock.loc[mask_stock].reset_index(drop=True)\n","df_reddit = df_reddit.loc[mask_reddit].reset_index(drop=True)\n","print(\"shapes: \")\n","print(df_stock.shape)\n","print(df_reddit.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zb09hBorEnhQ"},"outputs":[],"source":["# 3) Build full date-ticker grid\n","all_dates = pd.DataFrame({'date': df_stock[\"date\"].drop_duplicates().sort_values()})\n","tickers = df_reddit[\"ticker\"].unique()\n","full_index = pd.MultiIndex.from_product(\n","    [tickers, all_dates[\"date\"]],\n","    names=[\"ticker\", \"date\"]\n",").to_frame(index=False)\n","\n","# 4) Merge with Reddit data and forward fill\n","df_reddit_full = full_index.merge(df_reddit, on=[\"ticker\", \"date\"], how=\"left\")\n","df_reddit_full = df_reddit_full.sort_values([\"ticker\", \"date\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oy4ZBIp_EnhQ"},"outputs":[],"source":["# Forward fill missing sentiment and volume\n","#df_reddit_full[\"average_sentiment_score\"] = df_reddit_full.groupby(\"ticker\")[\"average_sentiment_score\"].ffill()\n","#df_reddit_full[\"comment_volume\"] = df_reddit_full.groupby(\"ticker\")[\"comment_volume\"].ffill()\n","\n","# 5) Create lag features (t-1)\n","df_reddit_full[\"comment_sentiment_lag1\"] = df_reddit_full.groupby(\"ticker\")[\"average_sentiment_score\"].shift(1)\n","df_reddit_full[\"comment_volume_lag1\"] = df_reddit_full.groupby(\"ticker\")[\"comment_volume\"].shift(1)\n","\n","# 6) Keep only lagged features for merge\n","df_reddit_lag = df_reddit_full[[\n","    \"date\", \"ticker\", \"comment_sentiment_lag1\", \"comment_volume_lag1\"\n","]]\n","\n","# 7) Merge with stock data\n","df_merged = df_stock.merge(\n","    df_reddit_lag,\n","    on=[\"date\", \"ticker\"],\n","    how=\"left\"\n",")\n","\n","# 8) Check result\n","print(df_merged.head(100))\n","print(f\"NaNs after merge:\\n{df_merged[['comment_sentiment_lag1', 'comment_volume_lag1']].isna().sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTBeu7xpEnhR"},"outputs":[],"source":["#REMOVE MRK ONLY ONE POINT OF DATA\n","df_merged = df_merged[df_merged[\"ticker\"] != \"MRK\"]\n","print(\"MRK in df_merged:\", \"mrk\" in df_merged[\"ticker\"].unique())\n","\n","missing = df_merged[df_merged[\"comment_sentiment_lag1\"].isna()]\n","print(missing[\"ticker\"].value_counts().head(10))  # Most affected tickers\n","print(missing.groupby(\"ticker\")[\"date\"].min().head(10))  # First missing date per ticker\n","\n","# See how many and what tickers/dates are affected\n","missing = df_merged[df_merged[\"comment_sentiment_lag1\"].isna()]\n","print(missing[[\"date\", \"ticker\"]].drop_duplicates().head(20))\n","print(f\"Missing rows: {len(missing)} / {len(df_merged)}\")\n","print(df_reddit_lag[\"date\"].nunique())\n","print(df_stock[\"date\"].nunique())\n","print(\"merged: \")\n","print(df_merged[\"date\"].nunique())\n","\n","#CHECK MISMATCH IN TICKER FORMAT?\n","stock_tickers = set(df_stock[\"ticker\"])\n","reddit_tickers = set(df_reddit[\"ticker\"])\n","print(\"Tickers in stock but not in reddit:\", stock_tickers - reddit_tickers)\n","print(\"Tickers in reddit but not in stock:\", reddit_tickers - stock_tickers)\n","\n","\n","\n","#HOW MANY NaNs?\n","print(f\"NaNs after merge:\\n{df_merged[['comment_sentiment_lag1', 'comment_volume_lag1']].isna().sum()}\")\n","\n","#REMOVE FIRST DAY BECAUSE OF T - 1\n","start, end = \"2024-02-03\", \"2025-04-08\"\n","masker = (df_merged['date'] >= start) & (df_merged['date'] <= end)\n","df_merged = df_merged.loc[masker].reset_index(drop=True)\n","\n","#HOW MANY NaNs?\n","print(f\"NaNs after merge:\\n{df_merged[['comment_sentiment_lag1', 'comment_volume_lag1']].isna().sum()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neW3pXmNEnhR"},"outputs":[],"source":["missing = df_merged[df_merged[\"comment_sentiment_lag1\"].isna()]\n","total_counts = df_merged[\"ticker\"].value_counts()\n","missing_counts = missing[\"ticker\"].value_counts()\n","coverage = (\n","    pd.DataFrame({\n","        \"total\": total_counts,\n","        \"missing\": missing_counts\n","    })\n","    .fillna(0)  # if some tickers have no missing values\n",")\n","coverage[\"missing_pct\"] = coverage[\"missing\"] / coverage[\"total\"]\n","good_tickers = coverage[coverage[\"missing_pct\"] <= 0.7].index\n","df_filtered = df_merged[df_merged[\"ticker\"].isin(good_tickers)].copy()\n","\n","print(f\"Kept {len(good_tickers)} tickers out of {len(coverage)}\")\n","print(f\"Filtered dataset shape: {df_filtered.shape}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBem16cUEnhS"},"outputs":[],"source":["print(df_filtered.head())\n","\n","print(df_filtered['ticker'].unique())\n","\n","\n","# Add missing indicators\n","df_filtered[\"reddit_sentiment_missing\"] = df_filtered[\"comment_sentiment_lag1\"].isna().astype(int)\n","# Fill NaNs with zeros\n","df_filtered[\"reddit_sentiment_lag1\"] = df_filtered[\"comment_sentiment_lag1\"].fillna(0.0)\n","df_filtered[\"reddit_volume_lag1\"] = df_filtered[\"comment_volume_lag1\"].fillna(0.0)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qWdCUxOEnhS"},"outputs":[],"source":["print(df_filtered.head())\n","print(df_filtered.shape)\n","df_filtered[['date','ticker','RealizedVol_3d','Target','reddit_sentiment_lag1','reddit_volume_lag1','reddit_sentiment_missing']].to_csv('yahooredditcombined.csv', index = False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}