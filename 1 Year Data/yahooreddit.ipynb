{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6c_45NBPEnhL"},"outputs":[],"source":["import pandas as pd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJ29YfLvEnhP","outputId":"33c4ac33-8a3f-409c-cea6-5535b8b93e68"},"outputs":[{"name":"stdout","output_type":"stream","text":["shapes: \n","(11248, 6)\n","(4347, 4)\n"]}],"source":["# 1) Load data\n","df_stock  = pd.read_csv(\"yfinance_final.csv\", parse_dates=[\"date\"])\n","df_reddit = pd.read_csv(\"clean_reddit_data.csv\", parse_dates=[\"date\"])\n","# Ensure date column is datetime\n","df_stock['date'] = pd.to_datetime(df_stock['date'])\n","df_reddit['date'] = pd.to_datetime(df_reddit['date'])\n","\n","# Uppercase and replace dots with dashes (like yfinance style)\n","df_stock[\"ticker\"] = df_stock[\"ticker\"].str.upper().str.replace('.', '-', regex=False)\n","df_reddit[\"ticker\"] = df_reddit[\"ticker\"].str.upper().str.replace('.', '-', regex=False)\n","\n","# 2) Define date range and filter both DataFrames\n","start, end = \"2024-02-02\", \"2025-04-08\"\n","mask_stock = (df_stock['date'] >= start) & (df_stock['date'] <= end)\n","mask_reddit = (df_reddit['date'] >= start) & (df_reddit['date'] <= end)\n","\n","\n","df_stock = df_stock.loc[mask_stock].reset_index(drop=True)\n","df_reddit = df_reddit.loc[mask_reddit].reset_index(drop=True)\n","print(\"shapes: \")\n","print(df_stock.shape)\n","print(df_reddit.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zb09hBorEnhQ"},"outputs":[],"source":["# 3) Build full date-ticker grid\n","all_dates = pd.DataFrame({'date': df_stock[\"date\"].drop_duplicates().sort_values()})\n","tickers = df_reddit[\"ticker\"].unique()\n","full_index = pd.MultiIndex.from_product(\n","    [tickers, all_dates[\"date\"]],\n","    names=[\"ticker\", \"date\"]\n",").to_frame(index=False)\n","\n","# 4) Merge with Reddit data and forward fill\n","df_reddit_full = full_index.merge(df_reddit, on=[\"ticker\", \"date\"], how=\"left\")\n","df_reddit_full = df_reddit_full.sort_values([\"ticker\", \"date\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oy4ZBIp_EnhQ","outputId":"586265a6-2ce0-4be7-8881-80388e595946"},"outputs":[{"name":"stdout","output_type":"stream","text":["         date ticker  RealizedVol_3d    Target     Volume       Close  \\\n","0  2024-02-02   AAPL        0.018846  0.016407  102518000  184.740829   \n","1  2024-02-02    ABT        0.007764  0.016340    7575400  109.364326   \n","2  2024-02-02   ACHR        0.033679  0.028469    3761800    4.940000   \n","3  2024-02-02    ACN        0.019876  0.019938    1590500  368.389832   \n","4  2024-02-02   AMZN        0.026586  0.051287  117154900  171.809998   \n","..        ...    ...             ...       ...        ...         ...   \n","95 2024-02-06    MCD        0.028023  0.019214    5927200  276.319733   \n","96 2024-02-06   META        0.125348  0.130211   21655200  452.586975   \n","97 2024-02-06    MRK        0.027245  0.003801    6153000  122.451012   \n","98 2024-02-06   MSFT        0.017698  0.016069   18382600  401.674866   \n","99 2024-02-06   NFLX        0.006264  0.003578    2840300  555.880005   \n","\n","    comment_sentiment_lag1  comment_volume_lag1  \n","0                      NaN                  NaN  \n","1                      NaN                  NaN  \n","2                      NaN                  NaN  \n","3                      NaN                  NaN  \n","4                      NaN                  NaN  \n","..                     ...                  ...  \n","95                     NaN                  NaN  \n","96                 0.07495                  4.0  \n","97                     NaN                  NaN  \n","98                     NaN                  NaN  \n","99                     NaN                  NaN  \n","\n","[100 rows x 8 columns]\n","NaNs after merge:\n","comment_sentiment_lag1    7783\n","comment_volume_lag1       7783\n","dtype: int64\n"]}],"source":["# Forward fill missing sentiment and volume\n","#df_reddit_full[\"average_sentiment_score\"] = df_reddit_full.groupby(\"ticker\")[\"average_sentiment_score\"].ffill()\n","#df_reddit_full[\"comment_volume\"] = df_reddit_full.groupby(\"ticker\")[\"comment_volume\"].ffill()\n","\n","# 5) Create lag features (t-1)\n","df_reddit_full[\"comment_sentiment_lag1\"] = df_reddit_full.groupby(\"ticker\")[\"average_sentiment_score\"].shift(1)\n","df_reddit_full[\"comment_volume_lag1\"] = df_reddit_full.groupby(\"ticker\")[\"comment_volume\"].shift(1)\n","\n","# 6) Keep only lagged features for merge\n","df_reddit_lag = df_reddit_full[[\n","    \"date\", \"ticker\", \"comment_sentiment_lag1\", \"comment_volume_lag1\"\n","]]\n","\n","# 7) Merge with stock data\n","df_merged = df_stock.merge(\n","    df_reddit_lag,\n","    on=[\"date\", \"ticker\"],\n","    how=\"left\"\n",")\n","\n","# 8) Check result\n","print(df_merged.head(100))\n","print(f\"NaNs after merge:\\n{df_merged[['comment_sentiment_lag1', 'comment_volume_lag1']].isna().sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTBeu7xpEnhR","outputId":"9875e1da-3e4b-404b-839c-0f58fab5e457"},"outputs":[{"name":"stdout","output_type":"stream","text":["MRK in df_merged: False\n","ticker\n","RTX     294\n","CRM     291\n","ISRG    290\n","CVX     289\n","NFLX    286\n","IBM     283\n","WMT     281\n","ORCL    279\n","TMO     277\n","INTU    274\n","Name: count, dtype: int64\n","ticker\n","AAPL    2024-02-02\n","ABT     2024-02-02\n","ACHR    2024-02-02\n","ACN     2024-02-02\n","AMZN    2024-02-02\n","BRK-B   2024-02-02\n","COST    2024-02-02\n","CRM     2024-02-02\n","CRWD    2024-02-02\n","CVX     2024-02-02\n","Name: date, dtype: datetime64[ns]\n","         date ticker\n","0  2024-02-02   AAPL\n","1  2024-02-02    ABT\n","2  2024-02-02   ACHR\n","3  2024-02-02    ACN\n","4  2024-02-02   AMZN\n","5  2024-02-02  BRK-B\n","6  2024-02-02   COST\n","7  2024-02-02    CRM\n","8  2024-02-02   CRWD\n","9  2024-02-02    CVX\n","10 2024-02-02   GOOG\n","11 2024-02-02  GOOGL\n","12 2024-02-02     HD\n","13 2024-02-02    IBM\n","14 2024-02-02   INTU\n","15 2024-02-02   ISRG\n","16 2024-02-02    JNJ\n","17 2024-02-02    JPM\n","18 2024-02-02     KO\n","19 2024-02-02    MCD\n","Missing rows: 7488 / 11419\n","296\n","296\n","merged: \n","296\n","Tickers in stock but not in reddit: set()\n","Tickers in reddit but not in stock: set()\n","NaNs after merge:\n","comment_sentiment_lag1    7488\n","comment_volume_lag1       7488\n","dtype: int64\n","NaNs after merge:\n","comment_sentiment_lag1    7451\n","comment_volume_lag1       7451\n","dtype: int64\n"]}],"source":["#REMOVE MRK ONLY ONE POINT OF DATA\n","df_merged = df_merged[df_merged[\"ticker\"] != \"MRK\"]\n","print(\"MRK in df_merged:\", \"mrk\" in df_merged[\"ticker\"].unique())\n","\n","missing = df_merged[df_merged[\"comment_sentiment_lag1\"].isna()]\n","print(missing[\"ticker\"].value_counts().head(10))  # Most affected tickers\n","print(missing.groupby(\"ticker\")[\"date\"].min().head(10))  # First missing date per ticker\n","\n","# See how many and what tickers/dates are affected\n","missing = df_merged[df_merged[\"comment_sentiment_lag1\"].isna()]\n","print(missing[[\"date\", \"ticker\"]].drop_duplicates().head(20))\n","print(f\"Missing rows: {len(missing)} / {len(df_merged)}\")\n","print(df_reddit_lag[\"date\"].nunique())\n","print(df_stock[\"date\"].nunique())\n","print(\"merged: \")\n","print(df_merged[\"date\"].nunique())\n","\n","#CHECK MISMATCH IN TICKER FORMAT?\n","stock_tickers = set(df_stock[\"ticker\"])\n","reddit_tickers = set(df_reddit[\"ticker\"])\n","print(\"Tickers in stock but not in reddit:\", stock_tickers - reddit_tickers)\n","print(\"Tickers in reddit but not in stock:\", reddit_tickers - stock_tickers)\n","\n","\n","\n","#HOW MANY NaNs?\n","print(f\"NaNs after merge:\\n{df_merged[['comment_sentiment_lag1', 'comment_volume_lag1']].isna().sum()}\")\n","\n","#REMOVE FIRST DAY BECAUSE OF T - 1\n","start, end = \"2024-02-03\", \"2025-04-08\"\n","masker = (df_merged['date'] >= start) & (df_merged['date'] <= end)\n","df_merged = df_merged.loc[masker].reset_index(drop=True)\n","\n","#HOW MANY NaNs?\n","print(f\"NaNs after merge:\\n{df_merged[['comment_sentiment_lag1', 'comment_volume_lag1']].isna().sum()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neW3pXmNEnhR","outputId":"5afbfdf0-4259-4e3d-dd83-75041f2c1aea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Kept 15 tickers out of 37\n","Filtered dataset shape: (4892, 8)\n"]}],"source":["missing = df_merged[df_merged[\"comment_sentiment_lag1\"].isna()]\n","total_counts = df_merged[\"ticker\"].value_counts()\n","missing_counts = missing[\"ticker\"].value_counts()\n","coverage = (\n","    pd.DataFrame({\n","        \"total\": total_counts,\n","        \"missing\": missing_counts\n","    })\n","    .fillna(0)  # if some tickers have no missing values\n",")\n","coverage[\"missing_pct\"] = coverage[\"missing\"] / coverage[\"total\"]\n","good_tickers = coverage[coverage[\"missing_pct\"] <= 0.7].index\n","df_filtered = df_merged[df_merged[\"ticker\"].isin(good_tickers)].copy()\n","\n","print(f\"Kept {len(good_tickers)} tickers out of {len(coverage)}\")\n","print(f\"Filtered dataset shape: {df_filtered.shape}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBem16cUEnhS","outputId":"d45e518c-3b6d-46c9-b36e-4b5f3536ee73"},"outputs":[{"name":"stdout","output_type":"stream","text":["         date ticker  RealizedVol_3d    Target    Volume       Close  \\\n","0  2024-02-05   AAPL        0.016407  0.009968  69668800  186.559891   \n","4  2024-02-05   AMZN        0.051287  0.043985  55081300  170.309998   \n","5  2024-02-05  BRK-B        0.010539  0.005626   3651900  390.760010   \n","6  2024-02-05   COST        0.012018  0.006546   2276900  707.288696   \n","10 2024-02-05   GOOG        0.045971  0.002081  29254400  144.246780   \n","\n","    comment_sentiment_lag1  comment_volume_lag1  \n","0                 0.252806                 33.0  \n","4                 0.318450                 12.0  \n","5                      NaN                  NaN  \n","6                 0.288800                  2.0  \n","10                0.663400                  2.0  \n","['AAPL' 'AMZN' 'BRK-B' 'COST' 'GOOG' 'GOOGL' 'HD' 'KO' 'META' 'MSFT'\n"," 'NVDA' 'PG' 'PLTR' 'PM' 'TSLA']\n"]}],"source":["print(df_filtered.head())\n","\n","print(df_filtered['ticker'].unique())\n","\n","\n","# Add missing indicators\n","df_filtered[\"reddit_sentiment_missing\"] = df_filtered[\"comment_sentiment_lag1\"].isna().astype(int)\n","# Fill NaNs with zeros\n","df_filtered[\"reddit_sentiment_lag1\"] = df_filtered[\"comment_sentiment_lag1\"].fillna(0.0)\n","df_filtered[\"reddit_volume_lag1\"] = df_filtered[\"comment_volume_lag1\"].fillna(0.0)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qWdCUxOEnhS","outputId":"52baa228-4682-4a51-abb6-9a6d50e978cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["         date ticker  RealizedVol_3d    Target    Volume       Close  \\\n","0  2024-02-05   AAPL        0.016407  0.009968  69668800  186.559891   \n","4  2024-02-05   AMZN        0.051287  0.043985  55081300  170.309998   \n","5  2024-02-05  BRK-B        0.010539  0.005626   3651900  390.760010   \n","6  2024-02-05   COST        0.012018  0.006546   2276900  707.288696   \n","10 2024-02-05   GOOG        0.045971  0.002081  29254400  144.246780   \n","\n","    comment_sentiment_lag1  comment_volume_lag1  reddit_sentiment_missing  \\\n","0                 0.252806                 33.0                         0   \n","4                 0.318450                 12.0                         0   \n","5                      NaN                  NaN                         1   \n","6                 0.288800                  2.0                         0   \n","10                0.663400                  2.0                         0   \n","\n","    reddit_sentiment_lag1  reddit_volume_lag1  \n","0                0.252806                33.0  \n","4                0.318450                12.0  \n","5                0.000000                 0.0  \n","6                0.288800                 2.0  \n","10               0.663400                 2.0  \n","(4892, 11)\n"]}],"source":["print(df_filtered.head())\n","print(df_filtered.shape)\n","df_filtered[['date','ticker','RealizedVol_3d','Target','reddit_sentiment_lag1','reddit_volume_lag1','reddit_sentiment_missing']].to_csv('yahooredditcombined.csv', index = False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}