{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WT_FbbPMoic9"},"outputs":[],"source":["# Main .py\n","!pip install pandas_market_calendars\n","!pip install praw\n","!pip install vaderSentiment\n","import praw\n","import pandas as pd\n","from datetime import datetime, timedelta\n","import json\n","import pandas_market_calendars as mcal\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"]},{"cell_type":"code","source":["\n","\n","#TOP 15 S AND P TICKERS CURRENTLY RANKED BY WEIGHT, REMOVED SOME TICKERS LIKE \"T,V,GS,GE,LLY,NOW,DIS\" because scraping comments would return incorrect pairings.\n","#Data wasn't found on all 50 TICKERS.\n","tickers = [\n","    \"NVDA\",  # Nvidia Corp.\n","    \"AAPL\",  # Apple\n","    \"COST\",  # Costco Wholesale Corp.\n","    \"TSLA\",  # Tesla, Inc.\n","    \"GOOGL\", # Alphabet Inc. Class A\n","    \"PM\",    # Philip Morris International Inc.\n","    \"MSFT\",  # Microsoft\n","    \"META\",  # Meta Platforms, Inc. Class A\n","    \"KO\",    # Coca-Cola Company\n","    \"AMZN\",  # Amazon.com Inc.\n","    \"PG\",    # Procter & Gamble Company\n","    \"BRK.B\", # Berkshire Hathaway Class B\n","    \"HD\",    # Home Depot, Inc.\n","    \"GOOG\",  # Alphabet Inc. Class C\n","    \"PLTR\"   # Palantir\n","]\n","ticker_to_company = {\n","    \"AAPL\":  [\"aapl\", \"apple\"],\n","    \"MSFT\":  [\"msft\", \"microsoft\"],\n","    \"NVDA\":  [\"nvda\", \"nvidia\"],\n","    \"AMZN\":  [\"amzn\", \"amazon\"],\n","    \"META\":  [\"meta\", \"facebook\", \"meta platforms\"],\n","    \"BRK.B\": [\"brk.b\", \"berkshire\", \"buffett\", \"berkshire hathaway\"],\n","    \"GOOGL\": [\"googl\", \"alphabet\", \"google\"],\n","    \"GOOG\":  [\"goog\", \"alphabet\", \"google\"],\n","    \"TSLA\":  [\"tsla\", \"tesla\", \"elon\"],\n","    \"COST\":  [\"cost\", \"costco\", \"costco wholesale\"],\n","    \"PG\":    [\"pg\", \"procter\", \"procter and gamble\"],\n","    \"HD\":    [\"hd\", \"home depot\"],\n","    \"KO\":    [\"ko\", \"coca cola\", \"coke\"],\n","    \"PM\":    [\"pm\", \"philip morris\"],\n","    \"PLTR\":  [\"pltr\", \"palantir\"]\n","}\n","\n","\n","\n","\n"],"metadata":{"id":"R0YzbyPSnUBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-Ri7AEao2iv"},"outputs":[],"source":["# Initialize Reddit API client using the credentials\n","def initialize_reddit_client():\n","    reddit = praw.Reddit(\n","        client_id='T_1IEZgS3e6ywIw4PxXjAQ',\n","        client_secret= 'oRZ-x7TyQanu5ehfVcVvSlvWMR514Q',\n","        user_agent=\"stock-volatility-sentiment-scraper by u/Osamabeenliftin69/Osamabeenliftin69\",\n","        )\n","    print(\"Logged in.\")\n","    return reddit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_s0atogmpMIx"},"outputs":[],"source":["import pandas as pd\n","from datetime import datetime, timedelta\n","import concurrent.futures\n","\n","def match_ticker(comment, mapping):\n","    \"\"\"\n","    Check if the comment contains a known ticker symbol or the associated company name.\n","    Return the normalized ticker if a match is found, otherwise None.\n","    \"\"\"\n","    for ticker, company in mapping.items():\n","        # Check if the ticker symbol itself exists in the comment\n","        if ticker.lower() in comment.lower():\n","            return ticker\n","\n","        # Handle the mapping value: if it's a list, iterate its items; otherwise, treat it as a string.\n","        if isinstance(company, list):\n","            for term in company:\n","                if term.lower() in comment.lower():\n","                    return ticker\n","        else:\n","            if company.lower() in comment.lower():\n","                return ticker\n","\n","    return None\n","\n","def process_post(post, two_years_ago, tickers):\n","    \"\"\"Process a single Reddit post and extract comment data for matching tickers.\"\"\"\n","    local_results = []\n","    post_date = datetime.fromtimestamp(post.created_utc)\n","    # Skip posts that are too old\n","    if post_date < two_years_ago:\n","        return local_results\n","    # Process only posts with the expected title parts\n","    relevant_keywords = [\"daily discussion\", \"stock discussion\", \"market discussion\"]\n","    if not any(keyword in post.title.lower() for keyword in relevant_keywords):\n","        return local_results\n","\n","    print(f\"Processing post: {post.title} on {post_date.date()}\")\n","\n","    # Expand the entire comment tree\n","    post.comments.replace_more(limit=0)\n","    comment_list = post.comments.list()\n","\n","    for comment in comment_list:\n","        # Some comments (e.g., deleted) lack a created timestamp\n","        if not hasattr(comment, \"created_utc\"):\n","            continue\n","        comment_date = datetime.fromtimestamp(comment.created_utc)\n","        if comment_date < two_years_ago:\n","            continue\n","        if comment_date.weekday() >= 5:\n","            continue\n","\n","        comment_text = comment.body\n","        # Check each ticker; stop after the first match to avoid duplicate counting.\n","        matched_ticker = match_ticker(comment_text, ticker_to_company)\n","        if matched_ticker:\n","            local_results.append({\n","                \"platform\": \"reddit\",\n","                \"ticker\": matched_ticker,\n","                \"post_title\": post.title,\n","                \"comment\": comment_text,\n","                \"comment_date\": comment_date.date(),\n","                \"comment_ups\": getattr(comment, \"ups\", None),\n","                \"comment_score\": getattr(comment, \"score\", None)\n","            })\n","    return local_results\n","\n","def scrape_reddit_data(reddit, tickers, limit):\n","    \"\"\"\n","    Scrapes Reddit posts and comments based on daily discussion threads and tickers,\n","    then returns a DataFrame. Uses concurrent processing to speed up scraping.\n","    \"\"\"\n","    if reddit is None:\n","        print(\"Reddit client is not initialized.\")\n","        return pd.DataFrame()\n","\n","    results = []\n","    two_years_ago = datetime.now() - timedelta(days=437)\n","    subreddits = ['wallstreetbets', 'stocks', 'investing']\n","\n","    # Expanded query to capture more than just \"Daily Discussion\"\n","    query = '(\"Daily Discussion\" OR \"Stock Discussion\" OR \"Market Discussion\" OR \"DD\")'\n","\n","    try:\n","        for subreddit_name in subreddits:\n","            subreddit = reddit.subreddit(subreddit_name)\n","            print(f\"Searching in r/{subreddit_name}...\")\n","            posts = list(subreddit.search(query, limit=limit, time_filter='all'))\n","\n","            # Use a ThreadPoolExecutor to process posts concurrently\n","            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n","                futures = [executor.submit(process_post, post, two_years_ago, tickers) for post in posts]\n","                for future in concurrent.futures.as_completed(futures):\n","                    results.extend(future.result())\n","\n","        df = pd.DataFrame(results)\n","        return df\n","\n","    except Exception as e:\n","        print(f\"Error scraping Reddit data: {e}\")\n","        return pd.DataFrame()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9utMpv6pRTV"},"outputs":[],"source":["reddit_client = initialize_reddit_client()\n","\n","\n","if reddit_client:\n","    df = scrape_reddit_data(reddit_client, tickers, limit=1000)\n","    if not df.empty:\n","        print(df.head())\n","        print(df.size)\n","        print(df.shape)\n","    else:\n","        print(\"No posts or comments found.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcPxHdg2p1CQ"},"outputs":[],"source":["tickers_per_day = df.groupby(\"comment_date\")[\"ticker\"].count().reset_index(name=\"ticker_mentions\")\n","print(\"Ticker Mentions Per Day:\")\n","print(tickers_per_day.to_string(index=False))\n","\n","# 2. Analyze Number of Different Tickers Total\n","# Count the number of unique tickers that appear in the dataset\n","unique_ticker_count = df[\"ticker\"].nunique()\n","unique_tickers = df[\"ticker\"].unique()\n","print(\"\\nTotal Number of Different Tickers:\", unique_ticker_count)\n","print(\"List of Unique Tickers:\")\n","print(unique_tickers)\n","\n","\n","# 3. Additional Analysis: Count of Comments Per Ticker\n","comments_per_ticker = df.groupby(\"ticker\")[\"comment\"].count().reset_index(name=\"comment_count\")\n","print(\"\\nComment Count per Ticker:\")\n","print(comments_per_ticker)\n","\n","#test ticker comments\n","testticker = 'CRWD'\n","filtered_df = df[df['ticker'] == testticker]\n","pd.set_option('display.max_colwidth', None)\n","# Print the first 5 comments for that ticker\n","print(f\"First 5 comments for {testticker}:\")\n","print(filtered_df['comment'].head(20))"]},{"cell_type":"code","source":["# Initialize VADER sentiment analyzer\n","sia = SentimentIntensityAnalyzer()\n","\n","# Compute a compound sentiment score for each comment\n","df[\"sentiment\"] = df[\"comment\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n","\n","# Convert comment_date column to datetime if not already\n","df[\"comment_date\"] = pd.to_datetime(df[\"comment_date\"])\n","# Create a new column with just the date portion\n","df[\"date\"] = df[\"comment_date\"].dt.date\n","\n","daily_summary = df.groupby([\"date\", \"ticker\"]).agg(\n","    comment_volume=(\"ticker\", \"size\"),  # Count of comments per group\n","    average_sentiment_score=(\"sentiment\", \"mean\")  # Average sentiment score per group\n",").reset_index()\n","\n","print(daily_summary.shape)\n","print(daily_summary.head(100))\n","\n","days_per_ticker = daily_summary.groupby(\"ticker\").size().sort_values(ascending=False)\n","print(days_per_ticker)\n"],"metadata":{"id":"7lpP9VoVFCgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["daily_summary.to_csv('daily_reddit_summaryasdf.csv', index=False)"],"metadata":{"id":"ER5DvfRpOcXo"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}